{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a568de8",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "<img src=\"https://mma.prnewswire.com/media/1095203/East_Tennessee_State_University_Logo.jpg?p=facebook\" width=200 height=200 />\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1 style=\"text-align: center\">CSCI 5037 - NLP & Text Analysis</h1>\n",
    "</div>\n",
    "\n",
    "# <center>Sentiment Analysis</center>\n",
    "\n",
    "**<center>Dr. Ahmad Al-Doulat </center>**\n",
    "<center>Department of Computing </center>\n",
    "<center>East Tennessee State University</center>\n",
    "\n",
    "<hr style=\"border:2px solid lightblue\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91af82",
   "metadata": {},
   "source": [
    "Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    "\n",
    "Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b949a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gzip\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib. cm as cm\n",
    "\n",
    "import nltk\n",
    "from nltk. sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import watermark\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664d48fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-09-25T15:37:56.561032-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.12\n",
      "IPython version      : 8.2.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 151 Stepping 2, GenuineIntel\n",
      "CPU cores   : 24\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15425b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8618750",
   "metadata": {},
   "source": [
    "# Word Counting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97bd10",
   "metadata": {},
   "source": [
    "We start by taking the simplest approach and simply counting positive and negative words. We'll use Hu and Liu's Lexicon from their 2004 KDD paper:\n",
    "\n",
    "https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f8a5c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a+', 'abound', 'abounds', ..., 'zenith', 'zest', 'zippy'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = np.loadtxt('positive-words.txt', dtype= 'str', comments=';')\n",
    "neg = np.loadtxt('negative-words.txt', dtype='str', comments=';')\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f15359a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2-faced', '2-faces', 'abnormal', ..., 'zealous', 'zealously',\n",
       "       'zombie'], dtype='<U24')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6aeb6",
   "metadata": {},
   "source": [
    "Create a dictionary and assign the valence to each positive and negative word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1de7604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence = {}\n",
    "\n",
    "for word in pos:\n",
    "    valence[word.lower()] = +1\n",
    "    \n",
    "for word in neg:\n",
    "    valence[word.lower()] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00848496",
   "metadata": {},
   "source": [
    "That now we can use to define a sentiment measuring function that returns the valence of a sentence or piece of text. Notice that we use the valence directly from the dictionary instead of treating positive and negative words separatly. This will prove useful later on ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5d2bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(words, valence):\n",
    "    word_count = 0\n",
    "    score = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in valence:\n",
    "            score += valence[word]\n",
    "            word_count += 1\n",
    "    return score/word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3930725",
   "metadata": {},
   "source": [
    "Now let's test our simple code with some simple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d890cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm very happy : 1.0\n",
      "The product is pretty annoying, and I hate it : -0.3333333333333333\n",
      "I'm sad : -1.0\n"
     ]
    }
   ],
   "source": [
    "texts = [\"I'm very happy\",\n",
    "         \"The product is pretty annoying, and I hate it\",         \n",
    "         \"I'm sad\",\n",
    "         ]\n",
    "\n",
    "for text in texts:\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    print(text, \":\", sentiment(words, valence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab496693",
   "metadata": {},
   "source": [
    "This is a bit surprising. One might expect the second sentence to be negative, after all \"pretty annoying\" and \"hate\" sound pretty negative. However, since each word is taken by itself, regardless of context we end up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d02afc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretty 1\n",
      "annoying -1\n",
      "hate -1\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(texts[1].lower())\n",
    "\n",
    "for word in words:\n",
    "    if word in valence:\n",
    "        print(word, valence[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9053b377",
   "metadata": {},
   "source": [
    "We'll see in a bit how to handle cases like this, but the solution requires two important changes to our current approach: modifier words and real valued weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d735f1",
   "metadata": {},
   "source": [
    "# Continuous Weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8fbc7",
   "metadata": {},
   "source": [
    "VADER is a state of the art sentiment analysis tool. Here we will use their excelent and well documented lexicon to explore non binary weights. Their approach is significantly more advanced than what we present here, but some of the fundamental ideas are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c86e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = pd.read_csv(\"vader_lexicon.txt\", sep='\\t', header=None, names= ['word', 'mean', 'std', 'scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d48062",
   "metadata": {},
   "source": [
    "The vader lexicon includes a lot of interesting information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92acdd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>grandeur</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.11355</td>\n",
       "      <td>[3, 2, 3, 2, 4, 1, 1, 4, 1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3354</th>\n",
       "      <td>gracile</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.78102</td>\n",
       "      <td>[1, 3, 2, 1, 2, 3, 2, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5342</th>\n",
       "      <td>proactive</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.87178</td>\n",
       "      <td>[2, 3, 3, 1, 1, 1, 1, 3, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>d:&lt;</td>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.97980</td>\n",
       "      <td>[-4, -4, -4, -1, -3, -3, -4, -2, -3, -4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>grandee</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.83066</td>\n",
       "      <td>[0, 1, 1, 0, 1, 1, 2, 1, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5473</th>\n",
       "      <td>rapeseeds</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.20416</td>\n",
       "      <td>[0, 0, 0, 0, 0, -4, 0, 0, -1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>compelling</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.94339</td>\n",
       "      <td>[1, 1, 1, 0, 1, 2, 2, -1, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>contemptuousness</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1.57797</td>\n",
       "      <td>[-3, -1, -2, 3, -2, 0, -1, -1, -2, -2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>dreary</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>0.48990</td>\n",
       "      <td>[-1, -1, -2, -1, -1, -2, -2, -1, -2, -1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>destruct</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.48990</td>\n",
       "      <td>[-3, -3, -3, -2, -2, -2, -3, -2, -2, -2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  mean      std  \\\n",
       "3368          grandeur   2.4  1.11355   \n",
       "3354           gracile   1.7  0.78102   \n",
       "5342         proactive   1.8  0.87178   \n",
       "227                d:<  -3.2  0.97980   \n",
       "3364           grandee   1.1  0.83066   \n",
       "5473         rapeseeds  -0.5  1.20416   \n",
       "1489        compelling   0.9  0.94339   \n",
       "1569  contemptuousness  -1.1  1.57797   \n",
       "2359            dreary  -1.4  0.48990   \n",
       "1995          destruct  -2.4  0.48990   \n",
       "\n",
       "                                        scores  \n",
       "3368            [3, 2, 3, 2, 4, 1, 1, 4, 1, 3]  \n",
       "3354            [1, 3, 2, 1, 2, 3, 2, 1, 1, 1]  \n",
       "5342            [2, 3, 3, 1, 1, 1, 1, 3, 1, 2]  \n",
       "227   [-4, -4, -4, -1, -3, -3, -4, -2, -3, -4]  \n",
       "3364            [0, 1, 1, 0, 1, 1, 2, 1, 3, 1]  \n",
       "5473          [0, 0, 0, 0, 0, -4, 0, 0, -1, 0]  \n",
       "1489           [1, 1, 1, 0, 1, 2, 2, -1, 2, 0]  \n",
       "1569    [-3, -1, -2, 3, -2, 0, -1, -1, -2, -2]  \n",
       "2359  [-1, -1, -2, -1, -1, -2, -2, -1, -2, -1]  \n",
       "1995  [-3, -3, -3, -2, -2, -2, -3, -2, -2, -2]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.iloc[np.random.randint(0, vader.shape[0], 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b7c1e",
   "metadata": {},
   "source": [
    "Smilies are also included and, in addition to the average sentiment of each word (in column 1) and it's standard deviation (in column 2) it provides the raw human generated scores in column 3. So that we may easily check (and possibly modify) their weights. To extract the raw scores for the word \"love\" we could simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5abd0184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word                                love\n",
      "mean                                 3.2\n",
      "std                                  0.4\n",
      "scores    [3, 3, 3, 3, 3, 3, 3, 4, 4, 3]\n",
      "Name: 4446, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(vader.iloc[4446])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bcec0e",
   "metadata": {},
   "source": [
    "From where we can also extract the individual scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7ccb465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 4, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "scores = eval(vader.iloc[4446][3])\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fe4f077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5adfe",
   "metadata": {},
   "source": [
    "And we can see that 8/10 people thought that the word love should receive a score of 3 and two others a score of 4. This gives us insight into how uniform the scores are. If for some reason, we thought that there was some problem with the 2 values of 4 or perhaps just not appropriate to our purposes we might discard them and recalculate the valence of the word.\n",
    "\n",
    "\n",
    "One justification for this might be the fact that the scores for the closely related word, \"loved\", are significantly different with a wider range of variation in the human scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6033239e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                               loved\n",
       "mean                                 2.9\n",
       "std                                  0.7\n",
       "scores    [3, 3, 4, 2, 2, 4, 3, 2, 3, 3]\n",
       "Name: 4447, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader.iloc[4447]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035893c7",
   "metadata": {},
   "source": [
    "Now we convert this dataset into a dictionary similar to the one we used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eb025fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_vader = dict(vader[['word', 'mean']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "690f484c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence_vader['loved']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe987dd",
   "metadata": {},
   "source": [
    "To use this new dictionary we just have to modify the arguments to the sentiment_modified function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f19b648a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment(nltk.word_tokenize(\"lt was not not very very good\"), valence_vader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0cc3c3",
   "metadata": {},
   "source": [
    "One important detail to keep in mind is that scores obtained through different methods are not comparable. In this example, the score of the sentence \"It was not not very very good\" went from 2.25 to 4.27 when we switched dictionaries. This is due not only to different levels of coverage in differnet dictionaries but also to differnet choices in the possible ranges of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef83c7",
   "metadata": {},
   "source": [
    "# NLTK Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd24ad7",
   "metadata": {},
   "source": [
    "nltk provides us with a VADER implementation that we can easily access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae07b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c72ac5",
   "metadata": {},
   "source": [
    "And we can even check that it is using the same file as we have been using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93759bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$:\t-1.5\t0.80623\t[-1, -1, -1, -1, -3, -1, -3, -1, -2, -1]\r\n",
      "%)\t-0.4\t1.0198\t[-1, 0, -1, 0, 0, -2, -1, 2, -1, 0]\r\n",
      "%-)\t-1.5\t1.43178\t[-2, 0, -2, -2, -1, 2, -2, -3, -2, -3]\r\n",
      "&-:\t-0.4\t1.42829\t[-3, -1, 0, 0, -1, -1, -1, 2, -1, 2]\r\n",
      "&:\t-0.7\t0.64031\t[0, -1, -1, -1, 1, -1, -1, -1, -1, -1]\r\n",
      "( '}{' )\t1.6\t0.66332\t[1, 2, 2, 1, 1, 2, 2, 1, 3, 1]\r\n",
      "(%\t-0.9\t0.9434\t[0, 0, 1, -1, -1, -1, -2, -2, -1, -2]\r\n",
      "('-:\t2.2\t1.16619\t[4, 1, 4, 3, 1, 2, 3, 1, 2, 1]\r\n",
      "(':\t2.3\t0.9\t[1, 3, 3, 2, 2, 4, 2, 3, 1, 2]\r\n",
      "((-:\t2.1\t0.53852\t[2, 2, 2, 1, 2, 3, 2, 2, 3, 2]\r\n",
      "(*\t1.1\t1.13578\t[2, 1, 1, -1, 1, 2, 2, -1, 2, 2]\r\n",
      "(-%\t-0.7\t1.26886\t[-1, 2, 0, -1, -1, -2, 0, 0, -3, -1]\r\n",
      "(-*\t1.3\t1.26886\t[4, 1, 2, 0, 2, -1, 1, 2, 1, 1]\r\n",
      "(-:\t1.6\t0.8\t[2, 2, 1, 3, 1, 1, 1, 3, 1, 1]\r\n",
      "(-:0\t2.8\t0.87178\t[3, 2, 3, 4, 3, 2, 3, 1, 4, 3]\r\n",
      "(-:<\t-0.4\t2.15407\t[-3, 3, -1, -1, 2, -1, -2, 3, -3, -1]\r\n",
      "(-:o\t1.5\t0.67082\t[3, 1, 1, 2, 2, 2, 1, 1, 1, 1]\r\n",
      "(-:O\t1.5\t0.67082\t[3, 1, 1, 2, 2, 2, 1, 1, 1, 1]\r\n",
      "(-:{\t-0.1\t1.57797\t[-2, -3, 1, -2, 1, 1, 0, 0, 2, 1]\r\n",
      "(-:|>*\t1.9\t0.83066\t\n"
     ]
    }
   ],
   "source": [
    "print(sentiment.lexicon_file[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f65a31",
   "metadata": {},
   "source": [
    "The sentiment analyzer expects a full sentence as input and returns scores along various dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ede2078d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4404}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores(\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a170fc57",
   "metadata": {},
   "source": [
    "The scores for negative, neutral and positive all add up to one, while compound can range from -1 to +1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d6fe6",
   "metadata": {},
   "source": [
    "# Modifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba09da",
   "metadata": {},
   "source": [
    "Our approach so far has ignored the possibility of modifiers. The first step towards improving our approach is to define a dictionary of modifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddab4989",
   "metadata": {},
   "outputs": [],
   "source": [
    "modifiers = {\n",
    "    \"very\": 1.5,\n",
    "    \"much\": 1.3,\n",
    "    \"not\": -1,\n",
    "    \"pretty\": 1.5,\n",
    "    \"somewhat\": 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1378789c",
   "metadata": {},
   "source": [
    "And to change our sentiment measuring function to take the modifiers into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4378e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_modified(text, valence, modifiers, verbose=False):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    word_count = 0\n",
    "    score = 0\n",
    "    ngrams = [[]]\n",
    "    \n",
    "    # generate ngrams\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        \n",
    "        if word in modifiers:\n",
    "            ngrams[-1].append(word)\n",
    "            continue\n",
    "            \n",
    "        if word in valence:\n",
    "            ngrams[-1].append(word)\n",
    "        \n",
    "        else:\n",
    "            if len(ngrams[-1]) > 0:\n",
    "                ngrams.append([])\n",
    "                \n",
    "    score = 0\n",
    "    \n",
    "    # Remove the trailing empty ngram if necessary \n",
    "    if len(ngrams[-1]) == 0:\n",
    "        ngrams = ngrams[:-1]\n",
    "        \n",
    "    for ngram in ngrams:\n",
    "        value = 1\n",
    "        \n",
    "        for word in ngram:\n",
    "            if word in modifiers:\n",
    "                value *= modifiers[word]\n",
    "            elif word in valence:\n",
    "                value *= valence[word]\n",
    "                \n",
    "        if verbose:\n",
    "            print(ngram, value)\n",
    "            \n",
    "        score += value\n",
    "    \n",
    "    return score/len(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31970e",
   "metadata": {},
   "source": [
    "This implementation is still relatively simple, but, as you can see, the results are already better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1456638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product is pretty annoying, and I hate it\n"
     ]
    }
   ],
   "source": [
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d731f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pretty', 'annoying'] -1.5\n",
      "['hate'] -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.25"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_modified(texts[1], valence, modifiers, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42f7a0",
   "metadata": {},
   "source": [
    "A more complete implementation would be more careful in handling the modifiers and would build larger ngrams so that cases like this one would also work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88964b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'very', 'good'] -1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_modified(\"It was not very good\", valence, modifiers, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0cb5a",
   "metadata": {},
   "source": [
    "And even more complex (and unrealistic) examples work fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64455056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'not', 'very', 'very', 'good'] 2.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.25"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_modified(\"It was not not very very good\", valence, modifiers, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874f709",
   "metadata": {},
   "source": [
    "Fortunately for us, the nltk vader implementation correctly handles modifiers under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb55f7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4404}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5350d2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.706, 'neu': 0.294, 'pos': 0.0, 'compound': -0.3412}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores(\"not good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cc1ffa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.238, 'pos': 0.762, 'compound': 0.4927}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores(\"very good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9456c4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.567, 'neu': 0.433, 'pos': 0.0, 'compound': -0.3865}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.polarity_scores(\"not very good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9734451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
